# E-Commerce Analytics Warehouse ğŸ“Š

A **production-ready, real-time data pipeline** for e-commerce analytics. Extract live product, order, and event data from **Fake Store API** (free, no-auth public API), transform and cleanse it, and load it into a star-schema data warehouse for analytics and reporting.

> **This project demonstrates real-time API data ingestion + batch processing + data warehouse best practices** in a fully containerized, production-ready setup.

---

## ğŸš€ Quick Start (5 minutes)

### Prerequisites
- Docker & Docker Compose installed
- Port 8080, 5050, 5432 available

### 1. Start All Services

```bash
cd ecomm-warehouse
docker-compose up -d
```

Wait 30 seconds for containers to initialize. Check status:
```bash
docker-compose ps
```

All services should show `Up` status. âœ…

### 2. Access the UIs

| Service | URL | Login |
|---------|-----|-------|
| **Airflow DAGs** | http://localhost:8080 | `airflow` / `airflow` |
| **Database UI** | http://localhost:5050 | `admin@example.com` / `admin` |
| **Database** | localhost:5432 | `airflow` / `airflow` |

### 3. Trigger a Real-Time API Run

The `ecomm_api_polling` DAG runs **automatically every 10 minutes**, OR trigger manually:

```bash
docker-compose exec airflow-scheduler airflow dags trigger ecomm_api_polling
```

### 4. Check Data

```bash
# Count orders in warehouse (via terminal)
docker-compose exec postgres psql -U airflow -d ecommerce_warehouse -c \
  "SELECT COUNT(*) as total_orders FROM warehouse.fact_orders;"

# Or query in pgAdmin UI â†’ Query Tool
```

**That's it!** The pipeline is ingesting real data from DummyJSON API and loading it into your warehouse. ğŸ‰

---

## ğŸ“Š What This Does

### Problem It Solves
You need a modern, scalable data pipeline that:
- âœ… Fetches **real-time data from APIs** (194 products with rich attributes)
- âœ… Handles **batch data** from local sources simultaneously
- âœ… Validates data quality before loading to warehouse
- âœ… Transforms raw data into **star schema** for analytics
- âœ… Runs reliably with **Airflow orchestration** and monitoring
- âœ… Tracks inventory, discounts, and customer behavior

### How It Works (High-Level)

```
DummyJSON API (Real-Time)           Local Files (Batch)
    â†“ (every 10 min)                      â†“ (daily)
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚     EXTRACT (api_polling DAG)               â”‚
â”‚  â”œâ”€ Poll /products (194 items)             â”‚
â”‚  â”œâ”€ Poll /carts (50 shopping carts)        â”‚
â”‚  â””â”€ Insert into raw.* tables               â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                    â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚     TRANSFORM (warehouse_daily DAG)         â”‚
â”‚  â”œâ”€ Type casting, deduplication            â”‚
â”‚  â”œâ”€ Clean null values, invalid records     â”‚
â”‚  â””â”€ Write to staging.* tables              â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                    â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚     QUALITY CHECKS                          â”‚
â”‚  â”œâ”€ Validate volume (min records)          â”‚
â”‚  â”œâ”€ Check uniqueness                       â”‚
â”‚  â”œâ”€ Validate timestamps                    â”‚
â”‚  â””â”€ Log failures to audit.* tables         â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                    â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚     LOAD (warehouse_daily DAG)              â”‚
â”‚  â”œâ”€ Upsert dimensions (users, products)    â”‚
â”‚  â”œâ”€ Insert facts (orders, events)          â”‚
â”‚  â””â”€ Calculate daily metrics                â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                    â†“
           Star Schema â­
      Ready for Analytics & BI
```

---

## ğŸ”Œ Data Sources

### **Fake Store API** (Real-Time) âœ¨ NEW!

Free, public API with realistic e-commerce data. **No authentication required.**

- **Base URL:** https://fakestoreapi.com
- **Endpoints Used:**
  - `GET /products` â†’ Products catalog
  - `GET /carts` â†’ Orders/carts
  - `GET /users` â†’ Customer data
- **Polling:** Every 10 minutes via `ecomm_api_polling` DAG
- **Data:** ~20 products, ~10 carts, ~10 users (sample data)

**Why Fake Store API?**
- âœ… Free, no API key needed
- âœ… Realistic e-commerce schema
- âœ… Stable, reliable endpoint
- âœ… Perfect for demos & learning
- âœ… Can replace with real API (Shopify, WooCommerce, etc.)

### **Local File Sources** (Batch)

For testing or batch ingestion:
- **Orders:** `/data/incoming/orders/YYYY-MM-DD/orders.json`
- **Events:** `/data/incoming/events/YYYY-MM-DD/events.csv`
- **Products:** `/data/incoming/products/products_YYYY-MM-DD.json` (generated Mondays)

Generate test data:
```bash
docker-compose exec airflow-scheduler python /opt/airflow/data/generators/generate_sample_data.py 2025-12-22 1
```

---

## ğŸ—ï¸ Architecture & Schema

### Database Layers

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ RAW LAYER (raw schema) - Immutable landing zone        â”‚
â”‚ â”œâ”€ raw.orders_json        JSONB bulk storage           â”‚
â”‚ â”œâ”€ raw.events_csv         CSV-normalized events        â”‚
â”‚ â””â”€ raw.products_json      JSONB product catalog        â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ STAGING LAYER (staging schema) - Cleaned & typed      â”‚
â”‚ â”œâ”€ staging.orders_clean   Typed, deduplicated orders  â”‚
â”‚ â”œâ”€ staging.events_clean   Normalized clickstream      â”‚
â”‚ â””â”€ staging.products_clean Enriched catalog            â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ WAREHOUSE LAYER (warehouse schema) - Star Schema â­   â”‚
â”‚                                                         â”‚
â”‚  DIMENSIONS:                                           â”‚
â”‚  â”œâ”€ dim_user      User attributes (city, signup_date) â”‚
â”‚  â”œâ”€ dim_product   Product catalog (price, category)   â”‚
â”‚  â””â”€ dim_date      Calendar dimension (1000+ dates)    â”‚
â”‚                                                         â”‚
â”‚  FACTS:                                                â”‚
â”‚  â”œâ”€ fact_orders   Order transactions (FK to dims)     â”‚
â”‚  â””â”€ fact_events   Behavioral events (user actions)    â”‚
â”‚                                                         â”‚
â”‚  METRICS:                                              â”‚
â”‚  â””â”€ daily_metrics Pipeline health (row counts, etc)   â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ AUDIT LAYER (audit schema) - Governance               â”‚
â”‚ â”œâ”€ data_quality_failures  Failed quality checks       â”‚
â”‚ â”œâ”€ run_logs               Detailed execution logs      â”‚
â”‚ â””â”€ schema_changes         DDL history                  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### Key Tables (Star Schema)

**Dimensions** (Lookup tables):
- `dim_user` (pk: user_id) â€” Customer info
- `dim_product` (pk: product_id) â€” Product catalog
- `dim_date` (pk: date_id) â€” Calendar (YYYYMMDD format)

**Facts** (Transaction tables):
- `fact_orders` (fk: user_id, product_id, date_id) â€” Order details
- `fact_events` (fk: user_id, date_id) â€” Behavioral events

---

## ğŸ”„ Pipelines (Airflow DAGs)

### 1. `ecomm_api_polling` ğŸ”„ Real-Time

**Schedule:** Every 10 minutes  
**Purpose:** Fetch live data from Fake Store API

```
START â†’ poll_orders â†’ â”€â”
                       â”œâ”€â†’ END
START â†’ poll_events  â”€â”€â”¤
                       â”‚
START â†’ poll_products â”€â”˜
```

**Tasks:**
- `poll_orders` â€” Fetch /carts, insert to raw.orders_json
- `poll_events` â€” Fetch /products + /carts, generate synthetic events
- `poll_products` â€” Fetch /products, insert to raw.products_json

### 2. `ecomm_warehouse_daily` ğŸ“Š Batch (Daily @ 2 AM UTC)

**Schedule:** Daily at 2:00 AM UTC  
**Purpose:** Transform raw data â†’ warehouse, run quality checks, generate metrics

```
EXTRACT                    TRANSFORM              QUALITY & LOAD
â”œâ”€ extract_orders    â”€â”€â”€â†’  transform_orders  â”€â”€â”
â”œâ”€ extract_events    â”€â”€â”€â†’  transform_events  â”€â”€â”¼â”€â†’ dq_checks â”€â”€â†’ load_dim_* â”€â”€â†’ load_fact_* â”€â”€â†’ metrics â”€â”€â†’ END
â””â”€ extract_products  â”€â”€â”€â†’  transform_products â”˜
```

**Key Tasks:**
- **Extract:** Read from local files or API
- **Transform:** Clean, validate, deduplicate
- **Quality Checks:** Validate row counts, uniqueness, future dates
- **Load Dimensions:** Upsert dim_user, dim_product, dim_date
- **Load Facts:** Insert fact_orders, fact_events (with FK validation)
- **Metrics:** Insert daily run summary to warehouse.daily_metrics

---

## ğŸ› ï¸ Technology Stack

| Component | Version | Purpose |
|-----------|---------|---------|
| **Apache Airflow** | 2.7.0 | Workflow orchestration & scheduling |
| **PostgreSQL** | 15 (Alpine) | Data warehouse & OLAP |
| **Python** | 3.11 | Data transformation & extraction |
| **Docker** | Latest | Containerization & deployment |
| **pgAdmin** | Latest | Database UI & query tool |

---

## ğŸ“‚ Project Structure

```
ecomm-warehouse/
â”œâ”€â”€ dags/
â”‚   â”œâ”€â”€ ecomm_api_polling.py           # Real-time API polling DAG ğŸ”„
â”‚   â””â”€â”€ ecomm_warehouse_daily.py       # Daily batch DAG ğŸ“Š
â”œâ”€â”€ src/
â”‚   â”œâ”€â”€ extract/
â”‚   â”‚   â”œâ”€â”€ orders.py                  # Extract orders from files
â”‚   â”‚   â”œâ”€â”€ events.py                  # Extract events from files
â”‚   â”‚   â”œâ”€â”€ products.py                # Extract products from files
â”‚   â”‚   â”œâ”€â”€ api_orders.py              # Extract orders from API âœ¨
â”‚   â”‚   â”œâ”€â”€ api_events.py              # Extract events from API âœ¨
â”‚   â”‚   â””â”€â”€ api_products.py            # Extract products from API âœ¨
â”‚   â”œâ”€â”€ transform/
â”‚   â”‚   â”œâ”€â”€ orders.py                  # Orders cleaning & validation
â”‚   â”‚   â”œâ”€â”€ events.py                  # Events normalization
â”‚   â”‚   â””â”€â”€ products.py                # Products enrichment
â”‚   â”œâ”€â”€ load/
â”‚   â”‚   â”œâ”€â”€ dimensions.py              # Load dim_user, dim_product, dim_date
â”‚   â”‚   â”œâ”€â”€ facts.py                   # Load fact_orders, fact_events
â”‚   â”‚   â””â”€â”€ metrics.py                 # Calculate daily metrics
â”‚   â”œâ”€â”€ quality/
â”‚   â”‚   â””â”€â”€ dq_checks.py               # Data quality validation
â”‚   â””â”€â”€ common/
â”‚       â”œâ”€â”€ db_utils.py                # Database connection & queries
â”‚       â””â”€â”€ logging_utils.py           # Logging setup
â”œâ”€â”€ sql/
â”‚   â”œâ”€â”€ 00_schemas.sql                 # Create schemas (raw, staging, warehouse, audit)
â”‚   â”œâ”€â”€ 01_raw_tables.sql              # Raw layer DDL
â”‚   â”œâ”€â”€ 02_staging_tables.sql          # Staging layer DDL
â”‚   â”œâ”€â”€ 03_warehouse_tables.sql        # Warehouse (star schema) DDL
â”‚   â”œâ”€â”€ 04_audit_tables.sql            # Audit layer DDL
â”‚   â””â”€â”€ 05_dim_date_seed.sql           # Populate dim_date with 3-year calendar
â”œâ”€â”€ data/
â”‚   â”œâ”€â”€ generators/
â”‚   â”‚   â””â”€â”€ generate_sample_data.py    # Generate test orders, events, products
â”‚   â””â”€â”€ incoming/
â”‚       â”œâ”€â”€ orders/                    # Daily order JSON files
â”‚       â”œâ”€â”€ events/                    # Daily event CSV files
â”‚       â””â”€â”€ products/                  # Weekly product JSON files
â”œâ”€â”€ docker-compose.yml                 # Docker setup (4 services)
â”œâ”€â”€ requirements.txt                   # Python dependencies
â”œâ”€â”€ README.md                          # This file
â””â”€â”€ docs/
    â”œâ”€â”€ ARCHITECTURE.md                # Detailed design docs
    â”œâ”€â”€ QUERIES.md                     # Example SQL queries
    â””â”€â”€ SCHEMA_DIAGRAM.md              # Visual schema
```

---

## ğŸš€ Common Tasks

### Generate Test Data & Run Pipeline

```bash
# Generate 1 day of test data (orders + events)
docker-compose exec airflow-scheduler python \
  /opt/airflow/data/generators/generate_sample_data.py 2025-12-22 1

# Trigger daily batch pipeline for that date
docker-compose exec airflow-scheduler airflow dags trigger \
  ecomm_warehouse_daily --exec-date 2025-12-22

# Watch the DAG in Airflow UI
# http://localhost:8080 â†’ DAGs â†’ ecomm_warehouse_daily â†’ Graph
```

### Manually Trigger API Polling

```bash
# Run the API polling DAG now (doesn't wait for 10-min schedule)
docker-compose exec airflow-scheduler airflow dags trigger ecomm_api_polling

# It will fetch latest data from Fake Store API immediately
# Check Airflow UI for progress and logs
```

### Query the Warehouse

**Via Terminal:**
```bash
docker-compose exec postgres psql -U airflow -d ecommerce_warehouse -c \
  "SELECT COUNT(*) as total_orders FROM warehouse.fact_orders;"
```

**Via pgAdmin UI (recommended):**
1. Open http://localhost:5050
2. Login: `admin@example.com` / `admin`
3. Click **Tools** â†’ **Query Tool**
4. Run SQL queries (see examples below)

### Example Analytics Queries

**Top Products by Revenue:**
```sql
SELECT 
  dp.product_name,
  COUNT(fo.order_id) as orders,
  SUM(fo.revenue) as revenue
FROM warehouse.fact_orders fo
JOIN warehouse.dim_product dp ON fo.product_id = dp.product_id
GROUP BY dp.product_id, dp.product_name
ORDER BY revenue DESC
LIMIT 10;
```

**Daily Sales Summary:**
```sql
SELECT 
  dd.date,
  COUNT(fo.order_id) as orders,
  SUM(fo.revenue) as daily_revenue,
  AVG(fo.revenue) as avg_order_value
FROM warehouse.fact_orders fo
JOIN warehouse.dim_date dd ON fo.date_id = dd.date_id
GROUP BY dd.date_id, dd.date
ORDER BY dd.date DESC
LIMIT 30;
```

**Event Funnel (view â†’ add to cart â†’ order):**
```sql
SELECT 
  fe.event_type,
  COUNT(DISTINCT fe.user_id) as unique_users,
  COUNT(*) as total_events
FROM warehouse.fact_events fe
GROUP BY fe.event_type
ORDER BY total_events DESC;
```

**Pipeline Health Check:**
```sql
SELECT 
  run_date,
  raw_orders_count,
  staging_orders_count,
  fact_orders_count,
  dq_failed_count,
  runtime_seconds
FROM warehouse.daily_metrics
ORDER BY run_date DESC
LIMIT 10;
```

---

## ğŸ”§ Configuration & Environment

### Default Credentials

```
Airflow:
  Username: airflow
  Password: airflow

pgAdmin:
  Email: admin@example.com
  Password: admin

PostgreSQL:
  User: airflow
  Password: airflow
  Database: ecommerce_warehouse
  Host: localhost (from host) / postgres (in Docker)
  Port: 5432
```

### Environment Variables

Create `.env` (optional, defaults work):
```env
POSTGRES_USER=airflow
POSTGRES_PASSWORD=airflow
POSTGRES_DB=ecommerce_warehouse
POSTGRES_PORT=5432
AIRFLOW_HOME=/opt/airflow
```

---

## ğŸ§ª Testing & Troubleshooting

### Check Everything is Running

```bash
docker-compose ps
# All 4 services should show "Up"
```

### View Logs

```bash
# Airflow scheduler logs (DAGs execution)
docker-compose logs -f airflow-scheduler

# PostgreSQL logs
docker-compose logs -f postgres

# View a specific DAG task log
docker-compose exec airflow-scheduler airflow tasks logs \
  ecomm_api_polling poll_orders $(date -u +'%Y-%m-%dT%H:00:00+00:00')
```

### Verify Data Pipeline

```bash
# Count records at each layer
docker-compose exec postgres psql -U airflow -d ecommerce_warehouse << EOF
SELECT 
  (SELECT COUNT(*) FROM raw.orders_json) as raw_orders,
  (SELECT COUNT(*) FROM staging.orders_clean) as staging_orders,
  (SELECT COUNT(*) FROM warehouse.fact_orders) as fact_orders,
  (SELECT COUNT(*) FROM raw.events_csv) as raw_events,
  (SELECT COUNT(*) FROM warehouse.fact_events) as fact_events;
EOF
```

### Reset Everything

```bash
# Stop and remove all containers & data
docker-compose down -v

# Start fresh
docker-compose up -d
```

---

## ğŸ“ˆ Performance & Scaling

### Current Setup
- **Batch frequency:** Daily (can change)
- **Real-time polling:** Every 10 minutes
- **Storage:** Local PostgreSQL
- **Execution:** LocalExecutor (single machine)
- **Scale:** ~1,000+ orders/day, ~10,000+ events/day

### For Production

```
Current              â†’    Production
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
LocalExecutor        â†’    KubernetesExecutor (distributed tasks)
Local Postgres       â†’    RDS / CloudSQL (managed, HA)
10-min polling       â†’    1-5 min polling (or event-driven)
Docker Desktop       â†’    Kubernetes / ECS / GCP Cloud Run
Single machine       â†’    Auto-scaling clusters
```

**Upgrade path:**
1. Replace `postgres` with RDS (change connection string in `docker-compose.yml`)
2. Add `KubernetesExecutor` to Airflow config
3. Deploy to EKS / GKE / AKS
4. Add caching layer (Redis) for hot data
5. Partition large tables by date for query performance

---

## ğŸ¤ Contributing

Want to extend this project?

- Add new data sources (Shopify, Stripe, GA4, etc.)
- Implement ML transformations (anomaly detection, recommendations)
- Add BI dashboard integration (Metabase, Looker)
- Performance optimizations (partitioning, indexing)

---

## ğŸ“š Additional Resources

- [Apache Airflow Docs](https://airflow.apache.org/docs/)
- [PostgreSQL Star Schema Modeling](https://en.wikipedia.org/wiki/Star_schema)
- [Fake Store API Docs](https://fakestoreapi.com/)
- [Docker Compose Reference](https://docs.docker.com/compose/)

---

## ğŸ’¡ Key Learnings

This project teaches you:
- âœ… **Real-time data ingestion** from APIs
- âœ… **Batch processing** from multiple file formats
- âœ… **Data warehousing** with star schema design
- âœ… **Airflow orchestration** and DAG design
- âœ… **Data quality** validation and monitoring
- âœ… **Container orchestration** with Docker
- âœ… **SQL** for OLAP and analytics
- âœ… **Python** for data transformation

---

## ğŸ“„ License

MIT License - Use freely for learning and commercial projects!

---

**Built with â¤ï¸ for data engineers, analytics engineers, and Python developers.**

**Need help?** Check logs, review Airflow UI (http://localhost:8080), or query the database directly in pgAdmin.

**Happy data engineering!** ğŸ‰
